# Embeddings Demo - RAG from Scratch

## Basic Principle

This program demonstrates how to use **embeddings** to find similarities between a search text and a collection of documents.

## Flow Diagram

```mermaid
flowchart TD
    A[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L38">User Question: Animals that swim?</a>] --> B[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L42">Convert to Embedding Vector</a>]

    C[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L13">Text Chunks</a>] --> D[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L57">Convert Each to Embedding Vector</a>]


    B --> E[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L68">Calculate Cosine Similarity</a>]
    D --> E

    E --> F[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L78">Similarity > 0.65?</a>]
    F -->|Yes| G[✅ Good Match]
    F -->|No| H[❌ Poor Match]

    G --> I[<a href="/200-first-let-s-talk-about-rag/1-embeddings-distances/main.go#L72">Display Results</a>]
    H --> I

    classDef question fill:#e3f2fd,stroke:#0277bd,stroke-width:2px
    classDef chunks fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    classDef process fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef decision fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef result fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px

    class A question
    class C,C1,C2,C3,C4 chunks
    class B,D,E process
    class F decision
    class G,H,I result
```

## How it Works

```
Question: "Quels sont les animaux qui nagent ?"
    ↓
Convert to numerical vector (embedding)
    ↓
Compare with embeddings of each chunk:
• "Les écureuils grimpent dans les arbres"
• "Les truites nagent dans la rivière" ← High similarity ✅
• "Les grenouilles nagent dans l'étang" ← High similarity ✅
• "Les lapins courent dans le champ"
    ↓
Calculate cosine similarity for each chunk
    ↓
Display results with ✅/❌ based on threshold (0.65)
```

## Key Concepts

- **Embedding**: Converting text into a numerical vector that captures its meaning
- **Cosine Similarity**: Measure of proximity between two vectors (0 = different, 1 = identical)
- **Threshold**: Limit value (0.65) to determine if two texts are similar

## Expected Result

Sentences containing the word "nagent" (swim) should have high similarity with the question about swimming animals, while others will have lower similarity.

## Purpose

This technique is the foundation of **RAG (Retrieval-Augmented Generation)**: finding the most relevant documents before generating a response.