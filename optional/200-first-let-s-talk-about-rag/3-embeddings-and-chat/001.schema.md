# Complete RAG - Embeddings + Chat Completion
> RAG from scratch

## Basic Principle

This program demonstrates a complete **RAG (Retrieval-Augmented Generation)** system that combines similarity search using embeddings with response generation using a chat model. It processes data about Auvergne gastronomy.

## Flow Diagram

```mermaid
flowchart TD
    A[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L110">User Question: What is Pounti?</a>] --> B[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L118">STEP 2: Convert to Embedding</a>]

    C[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L15">Gastronomic Chunks</a>] --> D[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L82">STEP 1: Create Embeddings</a>]
    D --> E[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L92">Save to Vector Store</a>]

    B --> F[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L132">Create VectorRecord</a>]
    E --> G[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L136">STEP 3: SearchTopNSimilarities</a>]
    F --> G

    G --> H[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L138">Build Documents Context</a>]
    H --> I[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L151">STEP 4: Prepare Chat Messages</a>]

    I --> J[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L153">System Instructions</a>]
    I --> K[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L155">Documents Context</a>]
    I --> L[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L157">User Question</a>]

    J --> M[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L167">Chat Completion Streaming</a>]
    K --> M
    L --> M

    M --> N[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L169">Real-time Response Display</a>]
    N --> O[<a href="/200-first-let-s-talk-about-rag/3-embeddings-and-chat/main.go#L182">Save Vector Store</a>]

    classDef question fill:#e3f2fd,stroke:#0277bd,stroke-width:2px
    classDef chunks fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    classDef process fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef storage fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef chat fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef result fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px

    class A question
    class C chunks
    class B,D,F,H process
    class E,G,O storage
    class I,J,K,L,M chat
    class N result
```

## How it Works

```
Question: "Explique moi ce qu'est le Pounti ?"
    ↓
STEP 1: Create embeddings for Auvergne dishes
• Truffade → VectorRecord
• Aligot → VectorRecord
• Pounti → VectorRecord ← Exact match ✅
• Cantal → VectorRecord
    ↓
STEP 2: Convert question to embedding
    ↓
STEP 3: Search for similarity in vector store
→ Finds "Pounti" chunk with high similarity
    ↓
STEP 4: Build context for chat
• System instructions (SYSTEM_INSTRUCTIONS)
• Relevant documents found
• Original user question
    ↓
Chat Completion with streaming → Real-time response generation
```

## Key Concepts

- **RAG (Retrieval-Augmented Generation)**: Combination of vector search + text generation
- **Embeddings Model**: Model to convert text into numerical vectors
- **Chat Model**: Language model to generate final responses
- **Streaming Response**: Real-time response display chunk by chunk
- **System Instructions**: Instructions given to the model to guide its responses
- **Context Window**: Relevant documents provided to the model as context

## Configuration via Environment Variables

- `MODEL_RUNNER_BASE_URL`: Model server URL
- `EMBEDDING_MODEL`: Embeddings model to use
- `COOK_MODEL`: Chat model for responses
- `SYSTEM_INSTRUCTIONS`: System instructions for model behavior
- `TEMPERATURE`: Response creativity (0.0-1.0)
- `TOP_P`: Response diversity control
- `SIMILARITY_LIMIT`: Minimum similarity threshold
- `SIMILARITY_MAX_RESULTS`: Maximum number of documents to retrieve

## Complete RAG Flow

1. **Preparation**: Create embeddings for all documents
2. **Indexing**: Store in vector store with unique identifiers
3. **Query**: Convert user question to embedding
4. **Search**: Identify most relevant documents
5. **Augmentation**: Build enriched context for the model
6. **Generation**: Produce response based on context and question
7. **Persistence**: Save vector store for reuse

## Expected Result

For the Pounti question, the system should:
1. Identify the chunk describing Pounti as most relevant
2. Use this context to generate a complete and accurate response
3. Display the response in streaming for smooth user experience
4. Save the vector store to `vectorstore.json`

## Purpose

This system illustrates a production-ready RAG pipeline, capable of answering complex questions by leveraging a specialized knowledge base (here Auvergne gastronomy).