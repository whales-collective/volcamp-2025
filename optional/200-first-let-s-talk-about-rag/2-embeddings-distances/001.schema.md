# Embeddings demo with Vector Store
> RAG from Scratch

## Basic Principle

This program demonstrates the use of **embeddings** with an in-memory **vector store** to find similarities between a user question and a collection of documents, then retrieve the most relevant documents.

## Flow Diagram

```mermaid
flowchart TD
    A[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L73">User Question: Animals that swim?</a>] --> B[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L82">Convert to Embedding Vector</a>]

    C[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L14">Text Chunks</a>] --> D[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L44">Convert to Embeddings</a>]
    D --> E[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L55">Save to Vector Store</a>]

    B --> F[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L94">Create VectorRecord from Question</a>]
    E --> G[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L98">Search TopN Similarities</a>]
    F --> G

    G --> H[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L98">Similarity Threshold > 0.6?</a>]
    H -->|Yes| I[✅ Relevant Match]
    H -->|No| J[❌ Irrelevant Match]

    I --> K[<a href="/200-first-let-s-talk-about-rag/2-embeddings-distances/main.go#L101">Display Results</a>]
    J --> K

    classDef question fill:#e3f2fd,stroke:#0277bd,stroke-width:2px
    classDef chunks fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    classDef process fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef storage fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef decision fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef result fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px

    class A question
    class C chunks
    class B,D,F process
    class E,G storage
    class H decision
    class I,J,K result
```

## How it Works

```
Question: "Quels sont les animaux qui nagent ?"
    ↓
Convert to numerical vector (embedding)
    ↓
Store chunks in MemoryVectorStore:
• "Les écureuils grimpent dans les arbres" → VectorRecord
• "Les truites nagent dans la rivière" → VectorRecord  ← High similarity ✅
• "Les grenouilles nagent dans l'étang" → VectorRecord ← High similarity ✅
• "Les lapins courent dans le champ" → VectorRecord
    ↓
Search with SearchTopNSimilarities(threshold: 0.6, limit: 2)
    ↓
Display results with cosine similarity score
```

## Key Concepts

- **Vector Store**: In-memory data structure to efficiently store and search embeddings
- **VectorRecord**: Record containing the original text and its associated embedding
- **SearchTopNSimilarities**: Method to search for the N most similar documents with a minimum threshold
- **Similarity Threshold**: Limit value (0.6) to filter relevant matches
- **Results Limit**: Maximum number of results to return (2)

## Improvements over Basic Version

1. **Structured Vector Store**: Use of `MemoryVectorStore` structure to organize data
2. **Optimized Search**: `SearchTopNSimilarities` method with configurable threshold and limit
3. **Error Handling**: Better error management during save operations
4. **Flexibility**: Adjustable search parameters (threshold and number of results)

## Expected Result

Sentences containing the word "nagent" (swim) should have high similarity with the question about swimming animals, while others will have lower similarity. Only the top 2 results exceeding the 0.6 threshold will be displayed.

## Purpose

This approach improves upon the basic version by introducing a dedicated data structure for vector storage, paving the way for more sophisticated **RAG (Retrieval-Augmented Generation)** systems.